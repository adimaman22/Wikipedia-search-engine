{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00e032c",
   "metadata": {
    "id": "hWgiQS0zkWJ5"
   },
   "source": [
    "***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ac36d3a",
   "metadata": {
    "id": "c0ccf76b",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Worker_Count",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "cf88b954-f39a-412a-d87e-660833e735b6",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
      "cluster-9074  GCE       4                                             RUNNING  us-central1-a\r\n"
     ]
    }
   ],
   "source": [
    "# if the following command generates an error, you probably didn't enable \n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security â†’ Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf86c5",
   "metadata": {
    "id": "01ec9fd3"
   },
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf199e6a",
   "metadata": {
    "id": "32b3ec57",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Setup",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38a897f2",
   "metadata": {
    "id": "b10cc999",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-jar",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "8f93a7ec-71e0-49c1-fc81-9af385849a90",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 247882 Mar  7 09:13 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47900073",
   "metadata": {
    "id": "d3f86f11",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-pyspark-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72bed56b",
   "metadata": {
    "id": "5be6dc2a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-spark-version",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-9074-m.c.phonic-sunbeam-414514.internal:33829\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f80c14a1600>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "980e62a5",
   "metadata": {
    "id": "7adc1bf5",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bucket_name",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Put your bucket name below and make sure you can access it without an error\n",
    "bucket_name = '208899385irproject'\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name != 'graphframes.sh' and b.name.endswith(\"preprocessed.parquet\"):\n",
    "        paths.append(full_path+b.name)\n",
    "# print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac891c2",
   "metadata": {
    "id": "13ZX4ervQkku"
   },
   "source": [
    "***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from operator import add\n",
    "import hashlib\n",
    "from nltk.stem.porter import *\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "582c3f5e",
   "metadata": {
    "id": "c0b0f215"
   },
   "source": [
    "# Building an inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f2044",
   "metadata": {
    "id": "02f81c72"
   },
   "source": [
    "Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4c523e7",
   "metadata": {
    "id": "b1af29c9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=====================================================>(122 + 2) / 124]\r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "#rdd for each type of index that we need (title , text(body), anchor_text)\n",
    "# limited = parquetFile.limit(1000)\n",
    "# doc_title_pairs = parquetFile.limit(1000).select(\"title\", \"id\").rdd\n",
    "# doc_body_pairs = parquetFile.limit(1000).select(\"text\", \"id\").rdd\n",
    "# doc_anchor_pairs = parquetFile.limit(1000).select(\"anchor_text\", \"id\").rdd\n",
    "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
    "doc_body_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "doc_anchor_pairs = parquetFile.select(\"anchor_text\", \"id\").rdd\n",
    "\n",
    "\n",
    "# need to do little manipulation on the anchor text rdd\n",
    "def make_string_from_anchor(input_string :str) -> str:\n",
    "    pattern = r\"(?<=text=\\')[^']+(?=\\')\"\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    string_back = \"\"\n",
    "    for term in matches:\n",
    "        string_back += term + \" \"\n",
    "\n",
    "    return string_back\n",
    "doc_anchor_pairs = doc_anchor_pairs.map(lambda x: (Row(anchor_text=make_string_from_anchor(str(x[0]).lower()),id=x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e2971",
   "metadata": {
    "id": "f6375562"
   },
   "source": [
    "We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82881fbf",
   "metadata": {
    "id": "d89a7a9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:====================================================> (120 + 4) / 124]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6348910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count number of wiki pages\n",
    "n = parquetFile.count()\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61fd1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=6348910"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701811af",
   "metadata": {
    "id": "gaaIoFViXyTg"
   },
   "source": [
    "Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "121fe102",
   "metadata": {
    "id": "04371c88",
    "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted_index_gcp.py\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57c101a8",
   "metadata": {
    "id": "2d3285d8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/07 16:35:07 WARN SparkContext: The path /home/dataproc/inverted_index_gcp.py has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    }
   ],
   "source": [
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c259c402",
   "metadata": {
    "id": "2477a5b9"
   },
   "outputs": [],
   "source": [
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540c727",
   "metadata": {
    "id": "72bcf46a"
   },
   "source": [
    "# Func for the implementation of the invertIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3ad8fea",
   "metadata": {
    "id": "a4b6ee29",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-token2bucket",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "def token2bucket_id(token):\n",
    "  return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "def word_count(text, id):\n",
    "    ''' Count the frequency of each word in `text` (tf) that is not included in\n",
    "    `all_stopwords` and return entries that will go into our posting lists.\n",
    "    Parameters:\n",
    "    -----------\n",
    "      text: str\n",
    "        Text of one document\n",
    "      id: int\n",
    "        Document id\n",
    "    Returns:\n",
    "    --------\n",
    "      List of tuples\n",
    "        A list of (token, (doc_id, tf)) pairs\n",
    "        for example: [(\"Anarchism\", (12, 5)), ...]\n",
    "    '''\n",
    "# Initialize the stemmer inside the function\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Remove stopwords from the list of tokens\n",
    "    filtered_tokens = [token for token in tokens if token not in all_stopwords]\n",
    "\n",
    "    # Calculate the term frequency (tf) for each unique token\n",
    "    tf_dict = {}\n",
    "    for token in filtered_tokens:\n",
    "        if token in tf_dict:\n",
    "            tf_dict[token] += 1\n",
    "        else:\n",
    "            tf_dict[token] = 1\n",
    "\n",
    "    # Create a list of tuples in the form (token, (doc_id, tf))\n",
    "    result = [(token, (id, tf)) for token, tf in tf_dict.items()]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    ''' Returns a sorted posting list by wiki_id.\n",
    "    Parameters:\n",
    "    -----------\n",
    "      unsorted_pl: list of tuples\n",
    "        A list of (wiki_id, tf) tuples\n",
    "    Returns:\n",
    "    --------\n",
    "      list of tuples\n",
    "        A sorted posting list.\n",
    "    '''\n",
    "    if not unsorted_pl:\n",
    "        return []\n",
    "\n",
    "    # Sort the unsorted posting list by wiki_id\n",
    "    sorted_pl = sorted(unsorted_pl)\n",
    "\n",
    "    return sorted_pl\n",
    "\n",
    "def calculate_df(postings):\n",
    "  ''' Takes a posting list RDD and calculate the df for each token.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    postings: RDD\n",
    "      An RDD where each element is a (token, posting_list) pair.\n",
    "  Returns:\n",
    "  --------\n",
    "    RDD\n",
    "      An RDD where each element is a (token, df) pair.\n",
    "  '''\n",
    "  df_rdd = postings.map(lambda x: (x[0], len(set(doc_id for doc_id, _ in x[1]))))\n",
    "\n",
    "  return df_rdd\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, BASE_DIR):\n",
    "  ''' A function that partitions the posting lists into buckets, writes out\n",
    "  all posting lists in a bucket to disk, and returns the posting locations for\n",
    "  each bucket. Partitioning should be done through the use of `token2bucket`\n",
    "  above. Writing to disk should use the function  `write_a_posting_list`, a\n",
    "  static method implemented in inverted_index_colab.py under the InvertedIndex\n",
    "  class.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    postings: RDD\n",
    "      An RDD where each item is a (w, posting_list) pair.\n",
    "  Returns:\n",
    "  --------\n",
    "    RDD\n",
    "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
    "      posting locations maintain a list for each word of file locations and\n",
    "      offsets its posting list was written to. See `write_a_posting_list` for\n",
    "      more details.\n",
    "  '''    # Function to write a posting list to disk\n",
    "# Assign each posting to a specific bucket based on the token\n",
    "  bucketed_postings = postings.map(lambda posting: (token2bucket_id(posting[0]), posting))\n",
    "#   BASE_DIR = SparkFiles.getRootDirectory()\n",
    "  # Organize postings by their bucket, accumulating postings lists for each bucket\n",
    "  organized_by_bucket = bucketed_postings.groupByKey().map(lambda bucket_postings: (bucket_postings[0], list(bucket_postings[1])))\n",
    "  # Function to process each bucket's postings and write to disk\n",
    "  def process_and_write_bucket(bucket_with_posts):\n",
    "        \n",
    "      bucket_id, posts_list = bucket_with_posts\n",
    "      # Format required by the write_a_posting_list method\n",
    "      bucket_data = (bucket_id, posts_list)\n",
    "      return InvertedIndex.write_a_posting_list(bucket_data,BASE_DIR,bucket_name)\n",
    "\n",
    "  # Apply the writing function to each bucket and collect the posting locations\n",
    "  locations_rdd = organized_by_bucket.map(process_and_write_bucket)\n",
    "\n",
    "  return locations_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1aa65d",
   "metadata": {},
   "source": [
    "# Create Title dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11820683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in dict for each doc_id the title\n",
    "title_dict = {}\n",
    "for title, doc_id in doc_title_pairs.collect():\n",
    "    title_dict[doc_id] = title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c42c6",
   "metadata": {},
   "source": [
    "# Create doc_len dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1125802",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "def get_doc_len(text_raw, doc_id):\n",
    "    return doc_id, len(list(word.finditer(text_raw.lower())))\n",
    "\n",
    "# save the len for each title:\n",
    "pair = doc_title_pairs.map(lambda x : get_doc_len(x[0],x[1]))\n",
    "title_len = Counter()\n",
    "for doc_id,t_len in pair.collect():\n",
    "    title_len[doc_id] = t_len\n",
    "\n",
    "pair = doc_body_pairs.map(lambda x : get_doc_len(x[0],x[1]))\n",
    "doc_len = Counter()\n",
    "for doc_id,len_d in pair.collect():\n",
    "    doc_len[doc_id] = len_d\n",
    "\n",
    "with open(\"doc_len.pkl\", 'wb') as f:\n",
    "  pickle.dump(doc_len, f)\n",
    "# print(doc_len[60283633])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18095d",
   "metadata": {},
   "source": [
    "# Reading from bucket index and read posting loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cbecfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "TUPLE_SIZE = 6\n",
    "\n",
    "def read_posting_list(inverted, w, bucket_name):\n",
    "  with closing(InvertedIndex.MultiFileReader()) as reader:\n",
    "    locs = inverted.posting_locs[w]\n",
    "    b = reader.read(bucket_name, locs, inverted.df[w] * TUPLE_SIZE)\n",
    "    posting_list = []\n",
    "    for i in range(inverted.df[w]):\n",
    "      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
    "      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
    "      posting_list.append((doc_id, tf))\n",
    "    return posting_list\n",
    "\n",
    "def read_pkl_file_form_bucket(file_name, name_bucket):\n",
    "    \"\"\"\n",
    "        func that read pkl file from the bucket\n",
    "    Args:\n",
    "        name_bucket: name of the bucket\n",
    "        file_name: the name of the pkl file + dir : pagerank\\page_rank\n",
    "\n",
    "    Returns:\n",
    "            dict\n",
    "    \"\"\"\n",
    "    # access to the bucket\n",
    "    bucket = storage.Client().get_bucket(name_bucket)\n",
    "    blob = bucket.get_blob(f'{file_name}.pkl')\n",
    "    if blob:\n",
    "      with blob.open(\"rb\") as pkl_file:\n",
    "          return pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff983324",
   "metadata": {},
   "source": [
    "# Indexing Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d85e9894",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_inverted_index(part_name: str) ->None:\n",
    "#     BASE_DIR = SparkFiles.getRootDirectory()\n",
    "    \"\"\"\n",
    "     func that create for each type : body , title .... inverter index\n",
    "    Args:\n",
    "      part_name: String body ,title ,anchor\n",
    "\n",
    "    Returns: None\n",
    "\n",
    "    \"\"\"\n",
    "    word_counts = None\n",
    "    bucket_name_to_save = \"\"\n",
    "    if part_name == \"body\":\n",
    "      # word counts map\n",
    "      bucket_name_to_save = bucket_name\n",
    "      word_counts = doc_body_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "      BASE_DIR = 'posting_body'\n",
    "\n",
    "    elif part_name == \"title\":\n",
    "      # word counts map\n",
    "      bucket_name_to_save = bucket_name\n",
    "      word_counts = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "      BASE_DIR = 'posting_title'\n",
    "\n",
    "    elif part_name == \"anchor\":\n",
    "      # word counts map\n",
    "      bucket_name_to_save = \"/\"\n",
    "      word_counts = doc_anchor_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "    \n",
    "    postings_filtered = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "    # filtering postings and calculate df\n",
    "    if part_name == \"body\":\n",
    "        # word counts map\n",
    "        postings_filtered = postings_filtered.filter(lambda x: len(x[1])>50)\n",
    "\n",
    "    w2df = calculate_df(postings_filtered)\n",
    "    w2df_dict = w2df.collectAsMap()\n",
    "       # Create inverted index instance\n",
    "    inverted = InvertedIndex()\n",
    "    inverted.term_total = postings_filtered.flatMapValues(lambda x : x).map(lambda x: (x[0],x[1][1])).reduceByKey(add).collectAsMap()\n",
    "    if part_name == \"body\":\n",
    "          inverted.avg_doc_len = np.sum(np.array(list(doc_len.values()))) / n\n",
    "    elif part_name == \"title\":\n",
    "          inverted.avg_doc_len = np.sum(np.array(list(title_len.values()))) / n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # partition posting lists and write out\n",
    "    _ = partition_postings_and_write(postings_filtered,BASE_DIR).collect()\n",
    "    # collect all posting lists locations into one super-set\n",
    "    super_posting_locs = defaultdict(list)\n",
    "    for blob in client.list_blobs(bucket_name_to_save, prefix=BASE_DIR):\n",
    "      if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "      with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "          super_posting_locs[k].extend(v)\n",
    "    \n",
    "\n",
    "    # adding the title of all the docs\n",
    "    inverted.title = title_dict\n",
    "    if part_name == \"body\":\n",
    "        inverted.doc_len = doc_len\n",
    "    # save the len of the entire corpus\n",
    "    if part_name == \"title\":\n",
    "        inverted.title_len = title_len  # Adding document lengths for body part\n",
    "    inverted.n = n\n",
    "    # Adding the posting locations dictionary to the inverted index\n",
    "    inverted.posting_locs = super_posting_locs\n",
    "    # Add the token - df dictionary to the inverted index\n",
    "    inverted.df = w2df_dict\n",
    "    # write the global stats out\n",
    "    inverted.write_index('.', f'index_{part_name}')\n",
    "    # upload to gs\n",
    "    index_src = f\"index_{part_name}.pkl\"\n",
    "    index_dst = f'gs://{bucket_name_to_save}/{BASE_DIR}/{index_src}'\n",
    "    !gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "55c8764e",
   "metadata": {
    "id": "0b5d7296",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-index_construction",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://index_title.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][119.6 KiB/119.6 KiB]                                                \n",
      "Operation completed over 1 objects/119.6 KiB.                                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://index_body.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 57.1 KiB/ 57.1 KiB]                                                \n",
      "Operation completed over 1 objects/57.1 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# make all inverters index\n",
    "\n",
    "make_inverted_index(\"title\")\n",
    "# make_inverted_index(\"anchor\")\n",
    "make_inverted_index(\"body\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "545f14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_body = InvertedIndex.read_index('postings_gcp/',\"index_body\", bucket_name)\n",
    "index_title = InvertedIndex.read_index('postings_gcp/',\"index_title\", bucket_name)\n",
    "dox_len = InvertedIndex.read_index('',\"doc_len\", bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0adf81d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: dict_values([596, 410, 107, 2095, 18, 565, 231, 638, 837, 191, 195, 336, 137, 153, 241, 213, 81, 273, 50, 116, 296, 888, 149, 373, 56, 77, 118, 58, 311, 85, 54, 479, 26, 928, 45, 260, 33, 180, 39, 227, 926, 1001, 38, 422, 302, 209, 82, 211, 98, 37, 289, 334, 218, 55, 116, 504, 1198, 284, 531, 359, 74, 531, 233, 172, 455, 121, 462, 531, 234, 473, 47, 195, 455, 35, 561, 51, 335, 273, 60, 183, 24, 232, 1672, 136, 486, 251, 709, 42, 249, 139, 290, 72, 114, 286, 1444, 174, 119, 141, 2499, 69, 185, 298, 201, 789, 303, 1329, 74, 155, 185, 298, 339, 215, 209, 318, 235, 201, 194, 389, 399, 28, 130, 21, 54, 117, 128, 199, 222, 47, 70, 183, 570, 78, 120, 97, 77, 319, 167, 298, 314, 106, 32, 176, 284, 170, 450, 1446, 201, 33, 115, 101, 28, 575, 196, 604, 616, 344, 314, 76, 36, 73, 130, 168, 122, 1188, 3927, 46, 211, 77, 518, 82, 932, 132, 243, 557, 185, 225, 302, 165, 131, 192, 110, 408, 50, 151, 281, 92, 839, 649, 77, 40, 293, 68, 1573, 605, 478, 1729, 828, 140, 264, 36, 81, 845, 197, 257, 174, 225, 37, 264, 328, 174, 52, 54, 182, 887, 350, 228, 133, 53, 136, 513, 45, 635, 657, 859, 218, 107, 99, 57, 985, 126, 243, 181, 21, 122, 44, 55, 316, 44, 50, 232, 77, 43, 260, 534, 864, 47, 449, 153, 245, 43, 1195, 47, 391, 44, 44, 347, 53, 404, 162, 47, 161, 421, 59, 740, 73, 712, 44, 47, 149, 44, 47, 415, 288, 44, 1351, 194, 522, 44, 163, 51, 150, 1608, 169, 151, 379, 149, 255, 48, 269, 195, 549, 260, 161, 1074, 146, 104, 303, 1371, 116, 71, 916, 240, 102, 22, 260, 54, 188, 4999, 106, 74, 530, 431, 1061, 186, 950, 68, 1513, 693, 100, 120, 273, 342, 334, 264, 149, 413, 596, 545, 68, 641, 240, 495, 1890, 290, 23, 39, 163, 103, 130, 129, 46, 215, 206, 663, 37, 53, 320, 481, 200, 147, 272, 106, 66, 183, 251, 353, 35, 555, 44, 511, 145, 356, 85, 341, 110, 551, 183, 186, 423, 69, 375, 168, 807, 493, 149, 156, 71, 263, 323, 1325, 262, 276, 132, 321, 241, 231, 331, 56, 60, 57, 64, 264, 1432, 247, 48, 164, 242, 410, 283, 73, 37, 53, 739, 29, 161, 52, 60, 118, 52, 20, 253, 24, 19, 1193, 128, 27, 1320, 34, 990, 26, 19, 191, 79, 572, 111, 390, 1997, 68, 215, 57, 60, 262, 116, 124, 179, 182, 46, 213, 33, 319, 1206, 56, 44, 452, 95, 204, 204, 1510, 88, 341, 135, 225, 88, 93, 271, 335, 22, 20, 62, 2585, 176, 677, 268, 1270, 882, 407, 63, 561, 162, 348, 429, 905, 21, 1329, 275, 131, 377, 496, 85, 65, 173, 160, 254, 148, 44, 650, 55, 42, 903, 432, 58, 427, 259, 1088, 20, 84, 62, 19, 37, 224, 310, 44, 112, 116, 577, 43, 20, 20, 67, 42, 405, 191, 18, 67, 42, 20, 19, 19, 19, 43, 283, 167, 152, 30, 747, 106, 364, 376, 302, 291, 200, 330, 97, 381, 122, 91, 67, 195, 227, 42, 44, 32, 42, 119, 356, 106, 271, 299, 80, 59, 129, 40, 48, 42, 151, 132, 43, 42, 34, 708, 42, 63, 29, 137, 42, 524, 31, 34, 68, 241, 49, 208, 67, 152, 27, 54, 103, 300, 24, 225, 16, 227, 2803, 276, 661, 127, 287, 224, 94, 71, 392, 229, 955, 186, 2229, 295, 161, 184, 142, 169, 395, 424, 42, 42, 42, 226, 96, 42, 27, 377, 206, 130, 363, 427, 153, 330, 68, 1738, 428, 54, 292, 90, 50, 290, 739, 256, 1332, 44, 166, 530, 305, 74, 29, 58, 105, 226, 413, 782, 101, 86, 402, 162, 258, 187, 153, 429, 92, 308, 68, 85, 139, 259, 227, 97, 40, 143, 227, 75, 24, 74, 178, 1378, 427, 144, 7109, 22, 68, 543, 471, 82, 681, 36, 548, 116, 252, 553, 1484, 523, 584, 98, 1376, 207, 39, 234, 103, 3878, 413, 583, 162, 201, 310, 35, 194, 17, 706, 139, 185, 19, 224, 190, 157, 453, 267, 77, 95, 101, 1313, 1153, 64, 323, 71, 55, 218, 44, 52, 5, 590, 48, 200, 493, 63, 128, 263, 57, 33, 122, 313, 142, 275, 360, 38, 38, 339, 285, 38, 64, 94, 1390, 302, 501, 65, 483, 1161, 440, 143, 157, 284, 318, 241, 496, 288, 78, 122, 103, 214, 7323, 184, 179, 188, 211, 174, 284, 112, 49, 178, 338, 338, 581, 241, 31, 74, 205, 193, 249, 102, 455, 116, 68, 174, 157, 94, 325, 612, 225, 55, 2632, 793, 596, 768, 365, 37, 252, 703, 314, 183, 931, 188, 123, 168, 1063, 969, 315, 339, 900, 178, 89, 39, 219, 525, 320, 51, 241, 267, 418, 49, 137, 75, 281, 580, 123, 176, 295, 101, 778, 66, 227, 126, 153, 625, 44, 244, 51, 476, 47, 0, 47, 43, 1172, 51, 47, 48, 47, 91, 103, 115, 333, 56, 236, 138, 44, 45, 391, 22, 387, 16, 47, 47, 138, 94, 45, 478, 305, 46, 1065, 523, 30, 47, 414, 44, 147, 141, 476, 107, 23, 47, 47, 348, 44, 146, 157, 241, 129, 254, 1682, 1148, 145, 117, 146, 34, 529, 47, 185, 44, 142, 25, 256, 1695, 44, 168, 41, 176, 793, 45, 268, 45, 20, 254, 110, 120, 1056, 47, 47, 142, 39, 443, 47, 90, 44, 286, 444, 26, 495, 122, 431, 777, 1098, 120, 333, 236, 243, 184, 312, 126, 53, 64, 20, 304, 488, 327, 193, 203, 270, 77, 29, 67, 39, 66, 200, 125, 147, 61, 490, 166, 735, 107, 1326, 289, 551, 124, 462, 346, 362, 108, 1160, 172, 124, 737, 47, 276, 658, 245, 172, 87, 8577, 89, 321, 86, 865, 215, 367, 245, 764, 202, 332, 82, 1042, 175, 732, 44, 65, 1807, 186, 549, 270, 617, 654, 2752, 816, 293, 41]) of type <class 'dict_values'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_61781/4009589593.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdoc_len\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/lib/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36msum\u001B[0;34m(col)\u001B[0m\n\u001B[1;32m    287\u001B[0m     \u001B[0mAggregate\u001B[0m \u001B[0mfunction\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mreturns\u001B[0m \u001B[0mthe\u001B[0m \u001B[0msum\u001B[0m \u001B[0mof\u001B[0m \u001B[0mall\u001B[0m \u001B[0mvalues\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mexpression\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    288\u001B[0m     \"\"\"\n\u001B[0;32m--> 289\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_invoke_function_over_columns\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"sum\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    290\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36m_invoke_function_over_columns\u001B[0;34m(name, *cols)\u001B[0m\n\u001B[1;32m     91\u001B[0m     \u001B[0;32mand\u001B[0m \u001B[0mwraps\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;32mclass\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m~\u001B[0m\u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mColumn\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m     \"\"\"\n\u001B[0;32m---> 93\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_invoke_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_java_column\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     94\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     91\u001B[0m     \u001B[0;32mand\u001B[0m \u001B[0mwraps\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;32mclass\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m~\u001B[0m\u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mColumn\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m     \"\"\"\n\u001B[0;32m---> 93\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_invoke_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_java_column\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     94\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36m_to_java_column\u001B[0;34m(col)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0mjcol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_create_column_from_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m         raise TypeError(\n\u001B[0m\u001B[1;32m     66\u001B[0m             \u001B[0;34m\"Invalid argument, not a string or column: \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0;34m\"{0} of type {1}. \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Invalid argument, not a string or column: dict_values([596, 410, 107, 2095, 18, 565, 231, 638, 837, 191, 195, 336, 137, 153, 241, 213, 81, 273, 50, 116, 296, 888, 149, 373, 56, 77, 118, 58, 311, 85, 54, 479, 26, 928, 45, 260, 33, 180, 39, 227, 926, 1001, 38, 422, 302, 209, 82, 211, 98, 37, 289, 334, 218, 55, 116, 504, 1198, 284, 531, 359, 74, 531, 233, 172, 455, 121, 462, 531, 234, 473, 47, 195, 455, 35, 561, 51, 335, 273, 60, 183, 24, 232, 1672, 136, 486, 251, 709, 42, 249, 139, 290, 72, 114, 286, 1444, 174, 119, 141, 2499, 69, 185, 298, 201, 789, 303, 1329, 74, 155, 185, 298, 339, 215, 209, 318, 235, 201, 194, 389, 399, 28, 130, 21, 54, 117, 128, 199, 222, 47, 70, 183, 570, 78, 120, 97, 77, 319, 167, 298, 314, 106, 32, 176, 284, 170, 450, 1446, 201, 33, 115, 101, 28, 575, 196, 604, 616, 344, 314, 76, 36, 73, 130, 168, 122, 1188, 3927, 46, 211, 77, 518, 82, 932, 132, 243, 557, 185, 225, 302, 165, 131, 192, 110, 408, 50, 151, 281, 92, 839, 649, 77, 40, 293, 68, 1573, 605, 478, 1729, 828, 140, 264, 36, 81, 845, 197, 257, 174, 225, 37, 264, 328, 174, 52, 54, 182, 887, 350, 228, 133, 53, 136, 513, 45, 635, 657, 859, 218, 107, 99, 57, 985, 126, 243, 181, 21, 122, 44, 55, 316, 44, 50, 232, 77, 43, 260, 534, 864, 47, 449, 153, 245, 43, 1195, 47, 391, 44, 44, 347, 53, 404, 162, 47, 161, 421, 59, 740, 73, 712, 44, 47, 149, 44, 47, 415, 288, 44, 1351, 194, 522, 44, 163, 51, 150, 1608, 169, 151, 379, 149, 255, 48, 269, 195, 549, 260, 161, 1074, 146, 104, 303, 1371, 116, 71, 916, 240, 102, 22, 260, 54, 188, 4999, 106, 74, 530, 431, 1061, 186, 950, 68, 1513, 693, 100, 120, 273, 342, 334, 264, 149, 413, 596, 545, 68, 641, 240, 495, 1890, 290, 23, 39, 163, 103, 130, 129, 46, 215, 206, 663, 37, 53, 320, 481, 200, 147, 272, 106, 66, 183, 251, 353, 35, 555, 44, 511, 145, 356, 85, 341, 110, 551, 183, 186, 423, 69, 375, 168, 807, 493, 149, 156, 71, 263, 323, 1325, 262, 276, 132, 321, 241, 231, 331, 56, 60, 57, 64, 264, 1432, 247, 48, 164, 242, 410, 283, 73, 37, 53, 739, 29, 161, 52, 60, 118, 52, 20, 253, 24, 19, 1193, 128, 27, 1320, 34, 990, 26, 19, 191, 79, 572, 111, 390, 1997, 68, 215, 57, 60, 262, 116, 124, 179, 182, 46, 213, 33, 319, 1206, 56, 44, 452, 95, 204, 204, 1510, 88, 341, 135, 225, 88, 93, 271, 335, 22, 20, 62, 2585, 176, 677, 268, 1270, 882, 407, 63, 561, 162, 348, 429, 905, 21, 1329, 275, 131, 377, 496, 85, 65, 173, 160, 254, 148, 44, 650, 55, 42, 903, 432, 58, 427, 259, 1088, 20, 84, 62, 19, 37, 224, 310, 44, 112, 116, 577, 43, 20, 20, 67, 42, 405, 191, 18, 67, 42, 20, 19, 19, 19, 43, 283, 167, 152, 30, 747, 106, 364, 376, 302, 291, 200, 330, 97, 381, 122, 91, 67, 195, 227, 42, 44, 32, 42, 119, 356, 106, 271, 299, 80, 59, 129, 40, 48, 42, 151, 132, 43, 42, 34, 708, 42, 63, 29, 137, 42, 524, 31, 34, 68, 241, 49, 208, 67, 152, 27, 54, 103, 300, 24, 225, 16, 227, 2803, 276, 661, 127, 287, 224, 94, 71, 392, 229, 955, 186, 2229, 295, 161, 184, 142, 169, 395, 424, 42, 42, 42, 226, 96, 42, 27, 377, 206, 130, 363, 427, 153, 330, 68, 1738, 428, 54, 292, 90, 50, 290, 739, 256, 1332, 44, 166, 530, 305, 74, 29, 58, 105, 226, 413, 782, 101, 86, 402, 162, 258, 187, 153, 429, 92, 308, 68, 85, 139, 259, 227, 97, 40, 143, 227, 75, 24, 74, 178, 1378, 427, 144, 7109, 22, 68, 543, 471, 82, 681, 36, 548, 116, 252, 553, 1484, 523, 584, 98, 1376, 207, 39, 234, 103, 3878, 413, 583, 162, 201, 310, 35, 194, 17, 706, 139, 185, 19, 224, 190, 157, 453, 267, 77, 95, 101, 1313, 1153, 64, 323, 71, 55, 218, 44, 52, 5, 590, 48, 200, 493, 63, 128, 263, 57, 33, 122, 313, 142, 275, 360, 38, 38, 339, 285, 38, 64, 94, 1390, 302, 501, 65, 483, 1161, 440, 143, 157, 284, 318, 241, 496, 288, 78, 122, 103, 214, 7323, 184, 179, 188, 211, 174, 284, 112, 49, 178, 338, 338, 581, 241, 31, 74, 205, 193, 249, 102, 455, 116, 68, 174, 157, 94, 325, 612, 225, 55, 2632, 793, 596, 768, 365, 37, 252, 703, 314, 183, 931, 188, 123, 168, 1063, 969, 315, 339, 900, 178, 89, 39, 219, 525, 320, 51, 241, 267, 418, 49, 137, 75, 281, 580, 123, 176, 295, 101, 778, 66, 227, 126, 153, 625, 44, 244, 51, 476, 47, 0, 47, 43, 1172, 51, 47, 48, 47, 91, 103, 115, 333, 56, 236, 138, 44, 45, 391, 22, 387, 16, 47, 47, 138, 94, 45, 478, 305, 46, 1065, 523, 30, 47, 414, 44, 147, 141, 476, 107, 23, 47, 47, 348, 44, 146, 157, 241, 129, 254, 1682, 1148, 145, 117, 146, 34, 529, 47, 185, 44, 142, 25, 256, 1695, 44, 168, 41, 176, 793, 45, 268, 45, 20, 254, 110, 120, 1056, 47, 47, 142, 39, 443, 47, 90, 44, 286, 444, 26, 495, 122, 431, 777, 1098, 120, 333, 236, 243, 184, 312, 126, 53, 64, 20, 304, 488, 327, 193, 203, 270, 77, 29, 67, 39, 66, 200, 125, 147, 61, 490, 166, 735, 107, 1326, 289, 551, 124, 462, 346, 362, 108, 1160, 172, 124, 737, 47, 276, 658, 245, 172, 87, 8577, 89, 321, 86, 865, 215, 367, 245, 764, 202, 332, 82, 1042, 175, 732, 44, 65, 1807, 186, 549, 270, 617, 654, 2752, 816, 293, 41]) of type <class 'dict_values'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c52dee14",
   "metadata": {
    "id": "fc0667a9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a6d655c112e79c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31a516e2",
   "metadata": {
    "id": "yVjnTvQsegc-"
   },
   "outputs": [],
   "source": [
    "# Put your `generate_graph` function here\n",
    "def generate_graph(pages):\n",
    "  ''' Compute the directed graph generated by wiki links.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    pages: RDD\n",
    "      An RDD where each row consists of one wikipedia articles with 'id' and \n",
    "      'anchor_text'.\n",
    "  Returns:\n",
    "  --------\n",
    "    edges: RDD\n",
    "      An RDD where each row represents an edge in the directed graph created by\n",
    "      the wikipedia links. The first entry should the source page id and the \n",
    "      second entry is the destination page id. No duplicates should be present. \n",
    "    vertices: RDD\n",
    "      An RDD where each row represents a vetrix (node) in the directed graph \n",
    "      created by the wikipedia links. No duplicates should be present. \n",
    "  '''\n",
    "  edges= pages.flatMap(lambda k:map(lambda p:(k['id'],p[0]),k['anchor_text'])).distinct()\n",
    "  vertices_1= pages.map(lambda k:k[0])\n",
    "  vertices_2_anchor=pages.flatMap(lambda k:k[1])\n",
    "  vertices_from_anchor= vertices_2_anchor.map(lambda k:k[0])\n",
    "  vertices_union= vertices_1.union(vertices_from_anchor)\n",
    "  vertices= vertices_union.distinct().map(lambda k:(k,))\n",
    "\n",
    "  return edges, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6bc05ba3",
   "metadata": {
    "id": "db005700",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-PageRank",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 15:01:13 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://wikidata_preprocessed/*.\n",
      "java.io.IOException: Error accessing gs://wikidata_preprocessed/*\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1141) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1115) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$getFileStatus$9(GoogleHadoopFileSystemBase.java:1073) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics.trackDuration(GhfsStorageStatistics.java:102) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:1062) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1881) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n",
      "\tat scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.18.jar:?]\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562) ~[spark-sql_2.12-3.3.2.jar:3.3.2]\n",
      "\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n",
      "\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n",
      "\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat java.lang.Thread.run(Thread.java:829) ~[?:?]\n",
      "Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n",
      "GET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/*?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n",
      "{\n",
      "  \"code\" : 403,\n",
      "  \"errors\" : [ {\n",
      "    \"domain\" : \"global\",\n",
      "    \"location\" : \"Authorization\",\n",
      "    \"locationType\" : \"header\",\n",
      "    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n",
      "    \"reason\" : \"accountDisabled\"\n",
      "  } ],\n",
      "  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n",
      "}\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224) ~[gcs-connector-hadoop3-2.2.20.jar:?]\n",
      "\t... 26 more\n",
      "24/03/06 15:01:14 WARN TaskSetManager: Lost task 0.0 in stage 145.0 (TID 1982) (cluster-983c-w-2.c.phonic-sunbeam-414514.internal executor 14): java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream10_preprocessed.parquet\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n",
      "\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n",
      "GET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream10_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n",
      "{\n",
      "  \"code\" : 403,\n",
      "  \"errors\" : [ {\n",
      "    \"domain\" : \"global\",\n",
      "    \"location\" : \"Authorization\",\n",
      "    \"locationType\" : \"header\",\n",
      "    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n",
      "    \"reason\" : \"accountDisabled\"\n",
      "  } ],\n",
      "  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n",
      "}\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n",
      "\t... 23 more\n",
      "\n",
      "24/03/06 15:01:14 WARN TaskSetManager: Lost task 1.0 in stage 145.0 (TID 1983) (cluster-983c-w-2.c.phonic-sunbeam-414514.internal executor 14): java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream11_part2_preprocessed.parquet\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n",
      "\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n",
      "GET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream11_part2_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n",
      "{\n",
      "  \"code\" : 403,\n",
      "  \"errors\" : [ {\n",
      "    \"domain\" : \"global\",\n",
      "    \"location\" : \"Authorization\",\n",
      "    \"locationType\" : \"header\",\n",
      "    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n",
      "    \"reason\" : \"accountDisabled\"\n",
      "  } ],\n",
      "  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n",
      "}\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n",
      "\t... 23 more\n",
      "\n",
      "24/03/06 15:01:14 ERROR TaskSetManager: Task 0 in stage 145.0 failed 4 times; aborting job\n",
      "24/03/06 15:01:14 WARN TaskSetManager: Lost task 1.3 in stage 145.0 (TID 1989) (cluster-983c-w-2.c.phonic-sunbeam-414514.internal executor 14): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o830.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 145.0 failed 4 times, most recent failure: Lost task 0.3 in stage 145.0 (TID 1988) (cluster-983c-w-2.c.phonic-sunbeam-414514.internal executor 14): java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream10_preprocessed.parquet\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream10_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"location\" : \"Authorization\",\n    \"locationType\" : \"header\",\n    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n    \"reason\" : \"accountDisabled\"\n  } ],\n  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream10_preprocessed.parquet\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream10_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"location\" : \"Authorization\",\n    \"locationType\" : \"header\",\n    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n    \"reason\" : \"accountDisabled\"\n  } ],\n  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_18276/2868115596.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpages_links\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"gs://wikidata_preprocessed/*\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlimit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"id\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"anchor_text\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mpages_links\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mpaths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlimit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"id\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"anchor_text\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# construct the graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0medges\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgenerate_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpages_links\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mparquet\u001B[0;34m(self, *paths, **options)\u001B[0m\n\u001B[1;32m    362\u001B[0m         )\n\u001B[1;32m    363\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 364\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpaths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    365\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    366\u001B[0m     def text(\n",
      "\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    191\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o830.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 145.0 failed 4 times, most recent failure: Lost task 0.3 in stage 145.0 (TID 1988) (cluster-983c-w-2.c.phonic-sunbeam-414514.internal executor 14): java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream10_preprocessed.parquet\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream10_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"location\" : \"Authorization\",\n    \"locationType\" : \"header\",\n    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n    \"reason\" : \"accountDisabled\"\n  } ],\n  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:139)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Error accessing gs://wikidata_preprocessed/multistream10_preprocessed.parquet\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2231)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2121)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1072)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:957)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$6(HadoopFSUtils.scala:136)\n\tat scala.collection.immutable.Stream.map(Stream.scala:418)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$4(HadoopFSUtils.scala:126)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/wikidata_preprocessed/o/multistream10_preprocessed.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"location\" : \"Authorization\",\n    \"locationType\" : \"header\",\n    \"message\" : \"The billing account for the owning project is disabled in state closed\",\n    \"reason\" : \"accountDisabled\"\n  } ],\n  \"message\" : \"The billing account for the owning project is disabled in state closed\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2224)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "pages_links = spark.read.parquet(\"gs://wikidata_preprocessed/*\").limit(1000).select(\"id\", \"anchor_text\").rdd\n",
    "\n",
    "pages_links = spark.read.parquet(*paths).limit(1000).select(\"id\", \"anchor_text\").rdd\n",
    "# construct the graph\n",
    "edges, vertices = generate_graph(pages_links)\n",
    "# compute PageRank\n",
    "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
    "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
    "g = GraphFrame(verticesDF, edgesDF)\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
    "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
    "# pr = pr.sort(col('pagerank').desc())\n",
    "# save the page rank dict\n",
    "pr = pr.toPandas().to_dict()\n",
    "pr = pd.DataFrame(pr)\n",
    "dict_pr = dict(zip(pr.id, pr.pagerank))\n",
    "#save the dict in pkl\n",
    "with open('page_rank.pkl', 'wb') as file:\n",
    "  pickle.dump(dict_pr, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e9a610",
   "metadata": {
    "id": "7f39m5R5TzZ2"
   },
   "source": [
    "# Page view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b143b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-03 14:10:45--  https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71, 2620:0:861:3:208:80:154:71\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2503235912 (2.3G) [application/octet-stream]\n",
      "Saving to: â€˜pageviews-202108-user.bz2â€™\n",
      "\n",
      "ser.bz2              85%[================>   ]   1.99G  4.61MB/s    eta 72s    "
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "# Using user page views (as opposed to spiders and automated traffic) for the\n",
    "# month of August 2021\n",
    "pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
    "p = Path(pv_path)\n",
    "pv_name = p.name\n",
    "pv_temp = f'{p.stem}-4dedup.txt'\n",
    "pv_clean = f'{p.stem}.pkl'\n",
    "# Download the file (2.3GB)\n",
    "!wget -N $pv_path\n",
    "# Filter for English pages, and keep just two fields: article ID (3) and monthly\n",
    "# total number of page views (5). Then, remove lines with article id or page\n",
    "# view values that are not a sequence of digits.\n",
    "!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
    "# Create a Counter (dictionary) that sums up the pages views for the same\n",
    "# article, resulting in a mapping from article id to total page views.\n",
    "wid2pv = Counter()\n",
    "with open(pv_temp, 'rt') as f:\n",
    "  for line in f:\n",
    "    parts = line.split(' ')\n",
    "    wid2pv.update({int(parts[0]): int(parts[1])})\n",
    "# write out the counter as binary file (pickle it)\n",
    "with open(pv_clean, 'wb') as f:\n",
    "  pickle.dump(wid2pv, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94564be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_postings_and_write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b49a5d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hadoop/spark/tmp/spark-4b7034c5-04da-4fe2-90da-af781de92b22/userFiles-7d305c5c-39ff-483a-b319-2ab94cb55672'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkFiles.getRootDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c728550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment3_gcp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "87df3f972d4a7d70f51716522c6b4120ab3bac837cd9e63eab3623068c7989bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
